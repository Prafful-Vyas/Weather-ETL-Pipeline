Here is a **clean, properly formatted, ready-to-paste `README.md`** â€” no formatting issues:

---

# ğŸŒ¦ï¸ Weather Data Lakehouse (DuckDB + uv)

A production-style **Medallion Architecture** data pipeline built using:

* DuckDB (in-process analytical database)
* Hive-style partitioned Parquet
* `uv` as the package manager
* Bronze â†’ Silver â†’ Gold layered modeling

This project demonstrates modern data engineering practices using a lightweight, local-first lakehouse design.

---

# ğŸ— Architecture

The pipeline follows a Medallion (Bronze / Silver / Gold) architecture pattern:

```
data/   â†’ Bronze (raw weather data)
silver/ â†’ Cleaned & standardized data
gold/   â†’ Aggregated & analytics-ready datasets
```

---

# ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ data/              # Bronze layer (raw weather data)
â”‚   â””â”€â”€ city=<city>/date=<YYYY-MM-DD>/weather.parquet
â”‚
â”œâ”€â”€ silver/            # Cleaned & transformed data
â”‚   â””â”€â”€ city=<city>/date=<YYYY-MM-DD>/data_0.parquet
â”‚
â”œâ”€â”€ gold/              # Aggregated analytics layer
â”‚   â””â”€â”€ city=<city>/date=<YYYY-MM-DD>/data_0.parquet
â”‚
â”œâ”€â”€ main.py            # Pipeline entrypoint
â”œâ”€â”€ sql-data-cleaning.ipynb
â”œâ”€â”€ requirements.txt
```

---

# ğŸ—‚ Partitioning Strategy

The dataset uses **Hive-style partitioning**:

```
city=Delhi/date=2026-02-13/weather.parquet
```

### Benefits

* Partition pruning
* Faster queries
* Reduced I/O
* Scalable storage layout
* Lakehouse-compatible design

DuckDB automatically detects partition columns (`city`, `date`) from folder names.

---

# ğŸ“¦ Package Management (uv)

This project uses **uv** instead of pip.

## Install dependencies

```bash
uv sync
```

If starting from scratch:

```bash
uv venv
uv pip install duckdb pandas pyarrow jupyter
```

Run the pipeline:

```bash
uv run python main.py
```

Run the notebook:

```bash
uv run jupyter notebook
```

---

# ğŸš€ Querying Partitioned Data with DuckDB

## Read Bronze Layer

```python
import duckdb

con = duckdb.connect()

df = con.execute("""
    SELECT *
    FROM read_parquet('data/**/*.parquet', hive_partitioning=true)
""").df()
```

DuckDB automatically extracts:

* `city`
* `date`

---

## Filter with Partition Pruning

```sql
SELECT *
FROM read_parquet('data/**/*.parquet', hive_partitioning=true)
WHERE city = 'Delhi'
  AND date = '2026-02-13';
```

Only the relevant partition is scanned.

---

# ğŸ¥ˆ Silver Layer (Cleaning Example)

```sql
CREATE OR REPLACE TABLE silver_weather AS
SELECT
    city,
    CAST(date AS DATE) AS date,
    temperature,
    humidity,
    wind_speed
FROM read_parquet('data/**/*.parquet', hive_partitioning=true)
WHERE temperature IS NOT NULL;
```

Export as partitioned Parquet:

```sql
COPY silver_weather
TO 'silver'
(FORMAT PARQUET, PARTITION_BY (city, date));
```

---

# ğŸ¥‡ Gold Layer (Aggregation Example)

```sql
CREATE OR REPLACE TABLE gold_daily_summary AS
SELECT
    city,
    date,
    AVG(temperature) AS avg_temp,
    MAX(temperature) AS max_temp,
    MIN(temperature) AS min_temp,
    AVG(humidity) AS avg_humidity
FROM read_parquet('silver/**/*.parquet', hive_partitioning=true)
GROUP BY city, date;
```

Export:

```sql
COPY gold_daily_summary
TO 'gold'
(FORMAT PARQUET, PARTITION_BY (city, date));
```

---

# âš¡ Why DuckDB?

* Vectorized execution engine
* Columnar processing
* In-process (no server required)
* Direct Parquet querying
* Automatic partition pruning
* Ideal for local analytics & pipelines

---

# ğŸ¯ What This Project Demonstrates

* Modern lakehouse architecture
* Hive-style partitioning
* SQL-first data transformations
* Bronze â†’ Silver â†’ Gold modeling
* Efficient analytical querying
* Reproducible pipelines using `uv`
* Production-style data engineering design

---

# ğŸ”® Future Improvements

* Incremental loads
* CLI interface
* Logging & monitoring
* Airflow orchestration
* Docker support
* CI/CD pipeline
* Cloud storage integration (S3/GCS/Azure)

---

# ğŸ“Œ Summary

This project implements a lightweight local lakehouse using DuckDB with a scalable partitioned layout and clean separation of data layers â€” mirroring production-grade data engineering workflows in a minimal, efficient setup.

